{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define TriangularCausalMask \n",
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "# Define FullAttention\n",
    "class FullAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(FullAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        scale = self.scale or 1. / sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
    "\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), A)\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "# Define DataEmbedding_inverted\n",
    "class DataEmbedding_inverted(nn.Module):\n",
    "    def __init__(self, c_in, hidden_size, dropout=0.1):\n",
    "        super(DataEmbedding_inverted, self).__init__()\n",
    "        self.value_embedding = nn.Linear(c_in, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(torch.cat([x, x_mark.permute(0, 2, 1)], 1))\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Define the iTransformer model \n",
    "class iTransformer(pl.LightningModule):\n",
    "    def __init__(self, h, input_size, n_series, hidden_size=512, n_heads=8, e_layers=2, d_ff=2048, factor=1, dropout=0.1, use_norm=True, lr=1e-3):\n",
    "        super(iTransformer, self).__init__()\n",
    "        self.h = h\n",
    "        self.input_size = input_size\n",
    "        self.n_series = n_series\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_heads = n_heads\n",
    "        self.e_layers = e_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.factor = factor\n",
    "        self.dropout = dropout\n",
    "        self.use_norm = use_norm\n",
    "        self.lr = lr\n",
    "\n",
    "        self.enc_embedding = DataEmbedding_inverted(input_size, self.hidden_size, self.dropout)\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=self.n_heads, dim_feedforward=self.d_ff, dropout=self.dropout)\n",
    "            for _ in range(self.e_layers)\n",
    "        ])\n",
    "\n",
    "        self.projector = nn.Linear(self.hidden_size, h, bias=True)\n",
    "\n",
    "    def forecast(self, x_enc):\n",
    "        if self.use_norm:\n",
    "            means = x_enc.mean(1, keepdim=True).detach()\n",
    "            x_enc = x_enc - means\n",
    "            stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x_enc /= stdev\n",
    "\n",
    "        enc_out = self.enc_embedding(x_enc, None)\n",
    "        for layer in self.encoder:\n",
    "            enc_out = layer(enc_out)\n",
    "\n",
    "        dec_out = self.projector(enc_out).permute(0, 2, 1)[:, :, :self.n_series]\n",
    "\n",
    "        if self.use_norm:\n",
    "            dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.h, 1))\n",
    "            dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.h, 1))\n",
    "\n",
    "        return dec_out\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        insample_y = windows_batch['insample_y'].unsqueeze(-1)  # Add an extra dimension\n",
    "        y_pred = self.forecast(insample_y)\n",
    "        y_pred = y_pred[:, -self.h:, :]\n",
    "        return y_pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        target = batch['insample_y'][:, -self.h:]\n",
    "        loss = F.mse_loss(output.squeeze(), target)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        target = batch['insample_y'][:, -self.h:]\n",
    "        val_loss = F.mse_loss(output.squeeze(), target)\n",
    "        self.log('val_loss', val_loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        mae = F.l1_loss(output.squeeze(), target)\n",
    "        self.log('val_mae', mae, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return {\"val_loss\": val_loss, \"val_mae\": mae}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "# Load the data\n",
    "file_path = '/Users/lennartzahn/Desktop/Masterarbeit/Notebooks (Transformer)/Cleaned_Data_EMA.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "# Filter for a single unique_id (replace 'unique_id_value' with the actual unique_id you want to filter)\n",
    "unique_id_value = 'User #25290'\n",
    "df_single_id = df[df['unique_id'] == unique_id_value]\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_size = int(len(df_single_id) * 0.8)\n",
    "df_train = df_single_id.iloc[:train_size]\n",
    "df_val = df_single_id.iloc[train_size:]\n",
    "\n",
    "# Define the dataset and dataloader\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, input_size, h):\n",
    "        self.df = df\n",
    "        self.input_size = input_size\n",
    "        self.h = h\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) - self.input_size - self.h + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        insample_y = self.df['y'][idx:idx+self.input_size].values.astype(np.float32)\n",
    "        return {\"insample_y\": torch.tensor(insample_y)}\n",
    "\n",
    "# Set parameters and create dataloader\n",
    "input_size = 16\n",
    "h = 8\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = TimeSeriesDataset(df_train, input_size, h)\n",
    "val_dataset = TimeSeriesDataset(df_val, input_size, h)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model with hyperparameters\n",
    "model = iTransformer(\n",
    "    h=h,\n",
    "    input_size=input_size,\n",
    "    n_series=1,\n",
    "    hidden_size=64,\n",
    "    n_heads=4,\n",
    "    e_layers=2,\n",
    "    d_ff=128,\n",
    "    dropout=0.1,\n",
    "    lr=1e-3,\n",
    ")\n",
    "\n",
    "# Define a PyTorch Lightning Callback for monitoring training\n",
    "class PrintCallback(pl.Callback):\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Epoch {trainer.current_epoch} - Training Loss: {trainer.callback_metrics['train_loss']:.4f} - Validation Loss: {trainer.callback_metrics['val_loss']:.4f} - Validation MAE: {trainer.callback_metrics['val_mae']:.4f}\")\n",
    "\n",
    "\n",
    "# Create a callback to log the loss\n",
    "class LossLogger(pl.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        train_loss = trainer.callback_metrics.get('train_loss')\n",
    "        if train_loss is not None:\n",
    "            self.train_losses.append(train_loss.item())\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        val_loss = trainer.callback_metrics.get('val_loss')\n",
    "        if val_loss is not None:\n",
    "            self.val_losses.append(val_loss.item())\n",
    "\n",
    "\n",
    "# Train the model\n",
    "loss_logger = LossLogger()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=[PrintCallback(), loss_logger],\n",
    "    default_root_dir='/Users/lennartzahn/Desktop/Masterarbeit/Notebooks (Transformer)',\n",
    "    log_every_n_steps=5\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            output = model(batch)\n",
    "            predictions.append(output.numpy())\n",
    "    return np.concatenate(predictions)\n",
    "\n",
    "# Example prediction\n",
    "predictions = predict(model, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and validation loss curves dpi=300 size 12x6\n",
    "plt.figure(figsize=(12, 6), dpi=300)\n",
    "plt.plot(loss_logger.train_losses, label='Training Loss')\n",
    "plt.plot(loss_logger.val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss per Epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the training and validation losses\n",
    "print(\"Train Losses:\", loss_logger.train_losses)\n",
    "print(\"Validation Losses:\", loss_logger.val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show array\n",
    "predictions\n",
    "\n",
    "# show array shape\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that predictions array only contains one prediction\n",
    "predictions = predictions[-1:]\n",
    "\n",
    "# plot the prediction for the next 8 steps with true and predicted values \n",
    "plt.figure(figsize=(10, 5), dpi=300)\n",
    "plt.plot(df_val['y'].values[-8:], label='True Values')\n",
    "plt.plot(predictions.squeeze(), label='Predictions')\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the mean absolute error\n",
    "mae = np.mean(np.abs(df_val['y'].values[-8:] - predictions.squeeze()))\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse = sqrt(np.mean((df_val['y'].values[-8:] - predictions.squeeze())**2))\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "# Calculate the mean absolute percentage error\n",
    "mape = np.mean(np.abs(df_val['y'].values[-8:] - predictions.squeeze()) / df_val['y'].values[-8:]) * 100\n",
    "print(\"Mean Absolute Percentage Error:\", mape)\n",
    "\n",
    "# Calculate the symmetric mean absolute percentage error\n",
    "smape = np.mean(np.abs(df_val['y'].values[-8:] - predictions.squeeze()) / (np.abs(df_val['y'].values[-8:]) + np.abs(predictions.squeeze()))) * 100\n",
    "print(\"Symmetric Mean Absolute Percentage Error:\", smape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prediction for the next 8 steps with true and predicted values and confidence intervals\n",
    "plt.figure(figsize=(10, 5), dpi=300)\n",
    "plt.plot(df_val['y'].values[-16:], label='True Values')\n",
    "plt.plot(np.arange(8, 16), predictions.squeeze(), label='Predictions')\n",
    "plt.axvline(x=8, color='r', linestyle='--', label='Start of Predictions')\n",
    "plt.fill_between(np.arange(8, 16), predictions.squeeze() - 1.96, predictions.squeeze() + 1.96, alpha=0.3)\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named 'df' with a column named 'Measurement'\n",
    "all_measurements_text = ' '.join(df['Measurement'].astype(str).tolist())\n",
    "print(all_measurements_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model   \n",
    "torch.save(model.state_dict(), '/Users/lennartzahn/Desktop/Masterarbeit/Notebooks (Transformer)/model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
